PreModel(
  (encoder): DeeperGCN(
    (node_encoder): Sequential(
      (0): Linear(in_features=121, out_features=128, bias=True)
      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (gcn_blocks): ModuleList(
      (0): ModuleList(
        (0): DeepGCNLayerV2(block=res+)
        (1): DeepGCNLayerV2(block=res+)
        (2): DeepGCNLayerV2(block=res+)
        (3): DeepGCNLayerV2(block=res+)
        (4): DeepGCNLayerV2(block=res+)
        (5): DeepGCNLayerV2(block=res+)
        (6): DeepGCNLayerV2(block=res+)
        (7): DeepGCNLayerV2(block=res+)
      )
    )
    (edge_encoder): Sequential(
      (0): Linear(in_features=11, out_features=128, bias=True)
      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (decoder): DeeperGCN(
    (node_encoder): Sequential(
      (0): Linear(in_features=128, out_features=121, bias=True)
      (1): LayerNorm((121,), eps=1e-05, elementwise_affine=True)
    )
    (gcn_blocks): ModuleList(
      (0): ModuleList(
        (0): DeepGCNLayerV2(block=res+)
      )
    )
    (edge_encoder): Sequential(
      (0): Linear(in_features=128, out_features=11, bias=True)
      (1): LayerNorm((11,), eps=1e-05, elementwise_affine=True)
    )
    (edge_decoder): Linear(in_features=121, out_features=11, bias=True)
  )
  (encoder2decoder_nodes): Linear(in_features=128, out_features=128, bias=False)
  (encoder2decoder_edges): Linear(in_features=128, out_features=128, bias=False)
)
loading processed data...
add degrees as node features for each sample...
in val dataloader...

val:Ep0000|| Loss: 0.85983

in train dataloader...

val:Ep0000|| Loss: 0.04257


trn:Ep0000|| Loss: 0.12905


val:Ep0001|| Loss: 0.02221


trn:Ep0001|| Loss: 0.02990


val:Ep0002|| Loss: 0.01759


trn:Ep0002|| Loss: 0.01991


val:Ep0003|| Loss: 0.01474


trn:Ep0003|| Loss: 0.01643


val:Ep0004|| Loss: 0.01344


trn:Ep0004|| Loss: 0.01443


val:Ep0005|| Loss: 0.01259


trn:Ep0005|| Loss: 0.01323


val:Ep0006|| Loss: 0.01174


trn:Ep0006|| Loss: 0.01240


val:Ep0007|| Loss: 0.01183


trn:Ep0007|| Loss: 0.01178


val:Ep0008|| Loss: 0.01076


trn:Ep0008|| Loss: 0.01128


val:Ep0009|| Loss: 0.01058


trn:Ep0009|| Loss: 0.01073


val:Ep0010|| Loss: 0.01021


trn:Ep0010|| Loss: 0.01035


val:Ep0011|| Loss: 0.00991


trn:Ep0011|| Loss: 0.01005


val:Ep0012|| Loss: 0.00942


trn:Ep0012|| Loss: 0.00964


val:Ep0013|| Loss: 0.00910


trn:Ep0013|| Loss: 0.00939


val:Ep0014|| Loss: 0.00908


trn:Ep0014|| Loss: 0.00922


val:Ep0015|| Loss: 0.00878


trn:Ep0015|| Loss: 0.00898


val:Ep0016|| Loss: 0.00898


trn:Ep0016|| Loss: 0.00877


val:Ep0017|| Loss: 0.00838


trn:Ep0017|| Loss: 0.00858


val:Ep0018|| Loss: 0.00847


trn:Ep0018|| Loss: 0.00844


val:Ep0019|| Loss: 0.00832


trn:Ep0019|| Loss: 0.00829


val:Ep0020|| Loss: 0.00811


trn:Ep0020|| Loss: 0.00833


val:Ep0021|| Loss: 0.00815


trn:Ep0021|| Loss: 0.00815


val:Ep0022|| Loss: 0.00783


trn:Ep0022|| Loss: 0.00793


val:Ep0023|| Loss: 0.00805


trn:Ep0023|| Loss: 0.00793


val:Ep0024|| Loss: 0.00759


trn:Ep0024|| Loss: 0.00786


val:Ep0025|| Loss: 0.00790


trn:Ep0025|| Loss: 0.00781


val:Ep0026|| Loss: 0.00768


trn:Ep0026|| Loss: 0.00777


val:Ep0027|| Loss: 0.00758


trn:Ep0027|| Loss: 0.00758


val:Ep0028|| Loss: 0.00755


trn:Ep0028|| Loss: 0.00763


val:Ep0029|| Loss: 0.00737


trn:Ep0029|| Loss: 0.00752


val:Ep0030|| Loss: 0.00758


trn:Ep0030|| Loss: 0.00753


val:Ep0031|| Loss: 0.00733


trn:Ep0031|| Loss: 0.00749


val:Ep0032|| Loss: 0.00731


trn:Ep0032|| Loss: 0.00746


val:Ep0033|| Loss: 0.00720


trn:Ep0033|| Loss: 0.00732


val:Ep0034|| Loss: 0.00721


trn:Ep0034|| Loss: 0.00729


val:Ep0035|| Loss: 0.00707


trn:Ep0035|| Loss: 0.00716


val:Ep0036|| Loss: 0.00727


trn:Ep0036|| Loss: 0.00715


val:Ep0037|| Loss: 0.00702


trn:Ep0037|| Loss: 0.00709


val:Ep0038|| Loss: 0.00712


trn:Ep0038|| Loss: 0.00705


val:Ep0039|| Loss: 0.00702


trn:Ep0039|| Loss: 0.00710


val:Ep0040|| Loss: 0.00688


trn:Ep0040|| Loss: 0.00699


val:Ep0041|| Loss: 0.00714


trn:Ep0041|| Loss: 0.00704


val:Ep0042|| Loss: 0.00703


trn:Ep0042|| Loss: 0.00690


val:Ep0043|| Loss: 0.00695


trn:Ep0043|| Loss: 0.00693


val:Ep0044|| Loss: 0.00690


trn:Ep0044|| Loss: 0.00687


val:Ep0045|| Loss: 0.00682


trn:Ep0045|| Loss: 0.00685


val:Ep0046|| Loss: 0.00678


trn:Ep0046|| Loss: 0.00684


val:Ep0047|| Loss: 0.00687


trn:Ep0047|| Loss: 0.00686


val:Ep0048|| Loss: 0.00700


trn:Ep0048|| Loss: 0.00680


val:Ep0049|| Loss: 0.00703

Epoch    51: reducing learning rate of group 0 to 1.0000e-04.

trn:Ep0049|| Loss: 0.00678


val:Ep0050|| Loss: 0.00640


trn:Ep0050|| Loss: 0.00652


val:Ep0051|| Loss: 0.00637


trn:Ep0051|| Loss: 0.00637


val:Ep0052|| Loss: 0.00644


trn:Ep0052|| Loss: 0.00636


val:Ep0053|| Loss: 0.00629


trn:Ep0053|| Loss: 0.00629


val:Ep0054|| Loss: 0.00632


trn:Ep0054|| Loss: 0.00629


val:Ep0055|| Loss: 0.00642


trn:Ep0055|| Loss: 0.00636


val:Ep0056|| Loss: 0.00642


trn:Ep0056|| Loss: 0.00633


val:Ep0057|| Loss: 0.00652


trn:Ep0057|| Loss: 0.00630


val:Ep0058|| Loss: 0.00642


trn:Ep0058|| Loss: 0.00632


val:Ep0059|| Loss: 0.00624


trn:Ep0059|| Loss: 0.00631


val:Ep0060|| Loss: 0.00627


trn:Ep0060|| Loss: 0.00633


val:Ep0061|| Loss: 0.00625


trn:Ep0061|| Loss: 0.00626


val:Ep0062|| Loss: 0.00629

Epoch    64: reducing learning rate of group 0 to 1.0000e-05.

trn:Ep0062|| Loss: 0.00626


val:Ep0063|| Loss: 0.00624


trn:Ep0063|| Loss: 0.00626


val:Ep0064|| Loss: 0.00633


trn:Ep0064|| Loss: 0.00631


val:Ep0065|| Loss: 0.00618


trn:Ep0065|| Loss: 0.00626


val:Ep0066|| Loss: 0.00617


trn:Ep0066|| Loss: 0.00622


val:Ep0067|| Loss: 0.00621


trn:Ep0067|| Loss: 0.00621


val:Ep0068|| Loss: 0.00634


trn:Ep0068|| Loss: 0.00623


val:Ep0069|| Loss: 0.00638


trn:Ep0069|| Loss: 0.00620


val:Ep0070|| Loss: 0.00629


trn:Ep0070|| Loss: 0.00624


val:Ep0071|| Loss: 0.00629


trn:Ep0071|| Loss: 0.00626


val:Ep0072|| Loss: 0.00621


trn:Ep0072|| Loss: 0.00619


val:Ep0073|| Loss: 0.00622


trn:Ep0073|| Loss: 0.00620


val:Ep0074|| Loss: 0.00635

Epoch    76: reducing learning rate of group 0 to 1.0000e-06.

trn:Ep0074|| Loss: 0.00617


val:Ep0075|| Loss: 0.00630


trn:Ep0075|| Loss: 0.00627


val:Ep0076|| Loss: 0.00631


trn:Ep0076|| Loss: 0.00620


val:Ep0077|| Loss: 0.00618


trn:Ep0077|| Loss: 0.00622


val:Ep0078|| Loss: 0.00622


trn:Ep0078|| Loss: 0.00625


val:Ep0079|| Loss: 0.00634


trn:Ep0079|| Loss: 0.00617


val:Ep0080|| Loss: 0.00625


trn:Ep0080|| Loss: 0.00625


val:Ep0081|| Loss: 0.00641


trn:Ep0081|| Loss: 0.00618


val:Ep0082|| Loss: 0.00633


trn:Ep0082|| Loss: 0.00613


val:Ep0083|| Loss: 0.00621

Epoch    85: reducing learning rate of group 0 to 1.0000e-07.

trn:Ep0083|| Loss: 0.00616


val:Ep0084|| Loss: 0.00622


trn:Ep0084|| Loss: 0.00619


val:Ep0085|| Loss: 0.00622


trn:Ep0085|| Loss: 0.00627


val:Ep0086|| Loss: 0.00631


trn:Ep0086|| Loss: 0.00621


val:Ep0087|| Loss: 0.00620


trn:Ep0087|| Loss: 0.00623


val:Ep0088|| Loss: 0.00633


trn:Ep0088|| Loss: 0.00621


val:Ep0089|| Loss: 0.00624


trn:Ep0089|| Loss: 0.00620


val:Ep0090|| Loss: 0.00621


trn:Ep0090|| Loss: 0.00619


val:Ep0091|| Loss: 0.00632


trn:Ep0091|| Loss: 0.00622


val:Ep0092|| Loss: 0.00617

Epoch    94: reducing learning rate of group 0 to 1.0000e-08.

trn:Ep0092|| Loss: 0.00622


val:Ep0093|| Loss: 0.00624


trn:Ep0093|| Loss: 0.00626


val:Ep0094|| Loss: 0.00626


trn:Ep0094|| Loss: 0.00620


val:Ep0095|| Loss: 0.00626


trn:Ep0095|| Loss: 0.00626


val:Ep0096|| Loss: 0.00621


trn:Ep0096|| Loss: 0.00626


val:Ep0097|| Loss: 0.00632


trn:Ep0097|| Loss: 0.00618


val:Ep0098|| Loss: 0.00620


trn:Ep0098|| Loss: 0.00618


val:Ep0099|| Loss: 0.00626

