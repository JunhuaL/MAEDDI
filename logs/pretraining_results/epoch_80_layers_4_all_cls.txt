PreModel(
  (encoder): DeeperGCN(
    (node_encoder): Sequential(
      (0): Linear(in_features=121, out_features=128, bias=True)
      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (gcn_blocks): ModuleList(
      (0): ModuleList(
        (0): DeepGCNLayerV2(block=res+)
        (1): DeepGCNLayerV2(block=res+)
        (2): DeepGCNLayerV2(block=res+)
        (3): DeepGCNLayerV2(block=res+)
      )
    )
    (edge_encoder): Sequential(
      (0): Linear(in_features=11, out_features=128, bias=True)
      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (decoder): DeeperGCN(
    (node_encoder): Sequential(
      (0): Linear(in_features=128, out_features=121, bias=True)
      (1): LayerNorm((121,), eps=1e-05, elementwise_affine=True)
    )
    (gcn_blocks): ModuleList(
      (0): ModuleList(
        (0): DeepGCNLayerV2(block=res+)
      )
    )
    (edge_encoder): Sequential(
      (0): Linear(in_features=128, out_features=11, bias=True)
      (1): LayerNorm((11,), eps=1e-05, elementwise_affine=True)
    )
    (edge_decoder): Linear(in_features=121, out_features=11, bias=True)
  )
  (encoder2decoder_nodes): Linear(in_features=128, out_features=128, bias=False)
  (encoder2decoder_edges): Linear(in_features=128, out_features=128, bias=False)
)
loading processed data...
add degrees as node features for each sample...
in val dataloader...

val:Ep0000|| Loss: 1.00995

in train dataloader...

val:Ep0000|| Loss: 0.03917


trn:Ep0000|| Loss: 0.11781


val:Ep0001|| Loss: 0.02269


trn:Ep0001|| Loss: 0.03008


val:Ep0002|| Loss: 0.01847


trn:Ep0002|| Loss: 0.02065


val:Ep0003|| Loss: 0.01634


trn:Ep0003|| Loss: 0.01780


val:Ep0004|| Loss: 0.01511


trn:Ep0004|| Loss: 0.01597


val:Ep0005|| Loss: 0.01403


trn:Ep0005|| Loss: 0.01488


val:Ep0006|| Loss: 0.01375


trn:Ep0006|| Loss: 0.01400


val:Ep0007|| Loss: 0.01296


trn:Ep0007|| Loss: 0.01340


val:Ep0008|| Loss: 0.01224


trn:Ep0008|| Loss: 0.01283


val:Ep0009|| Loss: 0.01195


trn:Ep0009|| Loss: 0.01250


val:Ep0010|| Loss: 0.01159


trn:Ep0010|| Loss: 0.01203


val:Ep0011|| Loss: 0.01126


trn:Ep0011|| Loss: 0.01167


val:Ep0012|| Loss: 0.01093


trn:Ep0012|| Loss: 0.01146


val:Ep0013|| Loss: 0.01099


trn:Ep0013|| Loss: 0.01115


val:Ep0014|| Loss: 0.01054


trn:Ep0014|| Loss: 0.01087


val:Ep0015|| Loss: 0.01024


trn:Ep0015|| Loss: 0.01080


val:Ep0016|| Loss: 0.01028


trn:Ep0016|| Loss: 0.01051


val:Ep0017|| Loss: 0.01003


trn:Ep0017|| Loss: 0.01027


val:Ep0018|| Loss: 0.00991


trn:Ep0018|| Loss: 0.01021


val:Ep0019|| Loss: 0.00978


trn:Ep0019|| Loss: 0.01000


val:Ep0020|| Loss: 0.00961


trn:Ep0020|| Loss: 0.00991


val:Ep0021|| Loss: 0.00954


trn:Ep0021|| Loss: 0.00975


val:Ep0022|| Loss: 0.00939


trn:Ep0022|| Loss: 0.00966


val:Ep0023|| Loss: 0.00963


trn:Ep0023|| Loss: 0.00959


val:Ep0024|| Loss: 0.00941


trn:Ep0024|| Loss: 0.00943


val:Ep0025|| Loss: 0.00895


trn:Ep0025|| Loss: 0.00943


val:Ep0026|| Loss: 0.00920


trn:Ep0026|| Loss: 0.00931


val:Ep0027|| Loss: 0.00904


trn:Ep0027|| Loss: 0.00929


val:Ep0028|| Loss: 0.00894


trn:Ep0028|| Loss: 0.00916


val:Ep0029|| Loss: 0.00891


trn:Ep0029|| Loss: 0.00917


val:Ep0030|| Loss: 0.00881


trn:Ep0030|| Loss: 0.00910


val:Ep0031|| Loss: 0.00906


trn:Ep0031|| Loss: 0.00898


val:Ep0032|| Loss: 0.00867


trn:Ep0032|| Loss: 0.00892


val:Ep0033|| Loss: 0.00878


trn:Ep0033|| Loss: 0.00893


val:Ep0034|| Loss: 0.00886


trn:Ep0034|| Loss: 0.00886


val:Ep0035|| Loss: 0.00883


trn:Ep0035|| Loss: 0.00883


val:Ep0036|| Loss: 0.00743


trn:Ep0036|| Loss: 0.00861


val:Ep0037|| Loss: 0.00722


trn:Ep0037|| Loss: 0.00737


val:Ep0038|| Loss: 0.00712


trn:Ep0038|| Loss: 0.00732


val:Ep0039|| Loss: 0.00706


trn:Ep0039|| Loss: 0.00725


val:Ep0040|| Loss: 0.00711


trn:Ep0040|| Loss: 0.00722


val:Ep0041|| Loss: 0.00699


trn:Ep0041|| Loss: 0.00714


val:Ep0042|| Loss: 0.00709


trn:Ep0042|| Loss: 0.00715


val:Ep0043|| Loss: 0.00707


trn:Ep0043|| Loss: 0.00713


val:Ep0044|| Loss: 0.00673


trn:Ep0044|| Loss: 0.00694


val:Ep0045|| Loss: 0.00662


trn:Ep0045|| Loss: 0.00668


val:Ep0046|| Loss: 0.00668


trn:Ep0046|| Loss: 0.00668


val:Ep0047|| Loss: 0.00661


trn:Ep0047|| Loss: 0.00663


val:Ep0048|| Loss: 0.00654


trn:Ep0048|| Loss: 0.00659


val:Ep0049|| Loss: 0.00658


trn:Ep0049|| Loss: 0.00651


val:Ep0050|| Loss: 0.00648


trn:Ep0050|| Loss: 0.00654


val:Ep0051|| Loss: 0.00651


trn:Ep0051|| Loss: 0.00646


val:Ep0052|| Loss: 0.00640


trn:Ep0052|| Loss: 0.00646


val:Ep0053|| Loss: 0.00642


trn:Ep0053|| Loss: 0.00646


val:Ep0054|| Loss: 0.00632


trn:Ep0054|| Loss: 0.00642


val:Ep0055|| Loss: 0.00624


trn:Ep0055|| Loss: 0.00643


val:Ep0056|| Loss: 0.00636


trn:Ep0056|| Loss: 0.00648


val:Ep0057|| Loss: 0.00630


trn:Ep0057|| Loss: 0.00632


val:Ep0058|| Loss: 0.00638


trn:Ep0058|| Loss: 0.00639


val:Ep0059|| Loss: 0.00630


trn:Ep0059|| Loss: 0.00633


val:Ep0060|| Loss: 0.00627


trn:Ep0060|| Loss: 0.00630


val:Ep0061|| Loss: 0.00625


trn:Ep0061|| Loss: 0.00635


val:Ep0062|| Loss: 0.00618


trn:Ep0062|| Loss: 0.00632


val:Ep0063|| Loss: 0.00606


trn:Ep0063|| Loss: 0.00626


val:Ep0064|| Loss: 0.00638


trn:Ep0064|| Loss: 0.00630


val:Ep0065|| Loss: 0.00614


trn:Ep0065|| Loss: 0.00625


val:Ep0066|| Loss: 0.00618


trn:Ep0066|| Loss: 0.00618


val:Ep0067|| Loss: 0.00623


trn:Ep0067|| Loss: 0.00624


val:Ep0068|| Loss: 0.00623


trn:Ep0068|| Loss: 0.00625


val:Ep0069|| Loss: 0.00608


trn:Ep0069|| Loss: 0.00618


val:Ep0070|| Loss: 0.00609


trn:Ep0070|| Loss: 0.00614


val:Ep0071|| Loss: 0.00603


trn:Ep0071|| Loss: 0.00614


val:Ep0072|| Loss: 0.00599

Epoch    74: reducing learning rate of group 0 to 1.0000e-04.

trn:Ep0072|| Loss: 0.00611


val:Ep0073|| Loss: 0.00579


trn:Ep0073|| Loss: 0.00578


val:Ep0074|| Loss: 0.00576


trn:Ep0074|| Loss: 0.00577


val:Ep0075|| Loss: 0.00585


trn:Ep0075|| Loss: 0.00574


val:Ep0076|| Loss: 0.00576


trn:Ep0076|| Loss: 0.00577


val:Ep0077|| Loss: 0.00578


trn:Ep0077|| Loss: 0.00572


val:Ep0078|| Loss: 0.00565


trn:Ep0078|| Loss: 0.00572


val:Ep0079|| Loss: 0.00573


trn:Ep0079|| Loss: 0.00569


val:Ep0080|| Loss: 0.00577


trn:Ep0080|| Loss: 0.00577


val:Ep0081|| Loss: 0.00572


trn:Ep0081|| Loss: 0.00574


val:Ep0082|| Loss: 0.00572


trn:Ep0082|| Loss: 0.00570


val:Ep0083|| Loss: 0.00569


trn:Ep0083|| Loss: 0.00575


val:Ep0084|| Loss: 0.00576


trn:Ep0084|| Loss: 0.00568


val:Ep0085|| Loss: 0.00567


trn:Ep0085|| Loss: 0.00576


val:Ep0086|| Loss: 0.00568


trn:Ep0086|| Loss: 0.00572


val:Ep0087|| Loss: 0.00557

Epoch    89: reducing learning rate of group 0 to 1.0000e-05.

trn:Ep0087|| Loss: 0.00574


val:Ep0088|| Loss: 0.00563


trn:Ep0088|| Loss: 0.00566


val:Ep0089|| Loss: 0.00552


trn:Ep0089|| Loss: 0.00558


val:Ep0090|| Loss: 0.00571


trn:Ep0090|| Loss: 0.00569


val:Ep0091|| Loss: 0.00571


trn:Ep0091|| Loss: 0.00562


val:Ep0092|| Loss: 0.00575


trn:Ep0092|| Loss: 0.00564


val:Ep0093|| Loss: 0.00564


trn:Ep0093|| Loss: 0.00564


val:Ep0094|| Loss: 0.00564


trn:Ep0094|| Loss: 0.00565


val:Ep0095|| Loss: 0.00562


trn:Ep0095|| Loss: 0.00566


val:Ep0096|| Loss: 0.00568


trn:Ep0096|| Loss: 0.00561


val:Ep0097|| Loss: 0.00562


trn:Ep0097|| Loss: 0.00565


val:Ep0098|| Loss: 0.00558

Epoch   100: reducing learning rate of group 0 to 1.0000e-06.

trn:Ep0098|| Loss: 0.00564


val:Ep0099|| Loss: 0.00565

